{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "The first part of this project is to use pre-existing models to transcribe audio files, while identifying who was speaking and when.\n",
    "\n",
    "- Speaker Diarisation\n",
    "- Speech to Text\n",
    "- Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import os \n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whisper requires the command line tool ffmpeg to be installed on the system. This needs to be done via a package manager like chocolatey for windows. This installs software packages at a system level. Options for other OS are listed in the README file. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kolea\\anaconda3\\Lib\\site-packages\\whisper\\__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "c:\\Users\\kolea\\anaconda3\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Thank you. Welcome to you both. It's wonderful to have you. It's an honor to have you both here tonight. We have inflation like very few people have ever seen before. Probably the worst in our nation's history. This has been a disaster for people, for the middle class, but for every class. Donald Trump left us the worst unemployment since the Great Depression. And what we have done is clean up Donald Trump's mess. She's a Marxist. Everybody knows she's a Marxist. Her father is a Marxist professor in economics. And he taught her well. But her vice presidential pick says abortion in the ninth month is absolutely fine. He also says execution after birth. It's execution no longer abortion because the baby is born is okay. And that's not okay with me. One does not have to abandon their face or deeply held back. Or deeply held beliefs to agree. The government and Donald Trump certainly should not be telling a woman what to do with her body. Pregnant women who want to carry a pregnancy to term, suffering from a miscarriage, being denied care in an emergency room because the healthcare providers are afraid they might go to jail. And she's bleeding out in a car in the parking lot. She didn't want that. Her husband didn't want that. In Springfield, they're eating the dogs. The people that came in. They're eating the cats. They're eating the pets of the people that live there. And this is what's happening in our country. Again, the Springfield City Manager says there's no evidence of that vice president here. So I'll let you respond to the rest of what you've heard. You talk about extreme. Are you now acknowledging that you've lost in 20-20? No, I don't acknowledge it at all. But, you know that. It was said, oh, we lost by our whisker. That was said sarcastically. Mr. President, thank you. Vice President Harris, you heard the president there tonight. He said he didn't say that. That he lost by whisker. So he still believes he did not lose the election. That was won by President Biden and yourself. Donald Trump was fired by 81 million people. So let's be clear about that. And clearly he is having a very difficult time processing that. World leaders are laughing at Donald Trump. I have talked with military leaders, some of whom work with you. And they say you're a disgrace. Understand what it would mean if Donald Trump were back in the White House with no guardrails. Because certainly we know now the court won't stop him. We know JD Vance is not going to stop him. It's up to the American people. We got to get you. This is the one that weaponized. Not me. She weaponized. I probably took a bullet to their head because of the things that they say about me. They talk about democracy. I'm a threat to democracy. There the threat to democracy. So I think you've heard tonight two very different visions for our country. One that is focused on the future. And the other that is focused on the past. And an attempt to take us backward. But we're not going back. They've had three and a half years to create jobs. And all the things we talked about. Why hasn't she done it? The worst president, the worst vice president in the history of our country.\n"
     ]
    }
   ],
   "source": [
    "#define path\n",
    "\n",
    "file_path = \"data/raw/TrumpHarrisDebate.mp3\"\n",
    "\n",
    "#select whisper model\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(file_path)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the transcribed text file\n",
    "\n",
    "transcriprtion = result[\"text\"]\n",
    "\n",
    "output_folder = \"data/processed\"\n",
    "output_file = os.path.join(output_folder, \"whisper_transcription.txt\")\n",
    "with open(output_file, \"w\") as file:\n",
    "  file.write(transcriprtion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code has transcribed the full interview to a txt file. However it does not identify the different speakers or separate their words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using whisper to detect what language the audiofile is.sample\n",
    "\n",
    "#get a 30s sample of the file\n",
    "#sample = whisper.load_audio(file_path)\n",
    "#sample = whisper.pad_or_trim(audio)\n",
    "\n",
    "\n",
    "# make log-Mel spectrogram and move to the same device as the model\n",
    "#mel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)\n",
    "\n",
    "# detect the spoken language\n",
    "#_, probs = model.detect_language(mel)\n",
    "#print(f\"Detected language: {max(probs, key=probs.get)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode the audio\n",
    "#options = whisper.DecodingOptions()\n",
    "#result = whisper.decode(model, mel, options)\n",
    "#print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kolea\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "#https://github.com/pyannote/pyannote-audio/blob/develop/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Authtoken import token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "c:\\Users\\kolea\\anaconda3\\Lib\\inspect.py:992: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\kolea\\.cache\\torch\\pyannote\\models--pyannote--segmentation\\snapshots\\c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b\\pytorch_model.bin`\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "c:\\Users\\kolea\\anaconda3\\Lib\\site-packages\\speechbrain\\utils\\fetching.py:151: UserWarning: Using SYMLINK strategy on Windows for fetching potentially requires elevated privileges and is not recommended. See `LocalStrategy` documentation.\n",
      "  warnings.warn(\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.5.1+cpu. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kolea\\anaconda3\\Lib\\site-packages\\speechbrain\\utils\\autocast.py:68: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n",
      "c:\\Users\\kolea\\anaconda3\\Lib\\site-packages\\speechbrain\\utils\\parameter_transfer.py:234: UserWarning: Requested Pretrainer collection using symlinks on Windows. This might not work; see `LocalStrategy` documentation. Consider unsetting `collect_in` in Pretrainer to avoid symlinking altogether.\n",
      "  warnings.warn(\n",
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
      "c:\\Users\\kolea\\anaconda3\\Lib\\site-packages\\speechbrain\\utils\\checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n",
      "c:\\Users\\kolea\\anaconda3\\Lib\\site-packages\\speechbrain\\processing\\features.py:1311: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stats = torch.load(path, map_location=device)\n",
      "c:\\Users\\kolea\\anaconda3\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=token)\n",
    "diarisation = pipeline(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPEAKER_02: 1.7s to 5.3s\n",
      "SPEAKER_00: 1.8s to 2.5s\n",
      "SPEAKER_00: 6.6s to 10.2s\n",
      "SPEAKER_01: 10.3s to 20.6s\n",
      "SPEAKER_02: 21.0s to 29.8s\n",
      "SPEAKER_01: 30.1s to 52.6s\n",
      "SPEAKER_02: 52.8s to 82.5s\n",
      "SPEAKER_01: 82.5s to 97.0s\n",
      "SPEAKER_00: 97.0s to 102.5s\n",
      "SPEAKER_01: 100.4s to 100.7s\n",
      "SPEAKER_02: 100.7s to 101.2s\n",
      "SPEAKER_02: 103.4s to 105.2s\n",
      "SPEAKER_00: 106.3s to 111.9s\n",
      "SPEAKER_01: 108.4s to 115.6s\n",
      "SPEAKER_00: 115.6s to 128.2s\n",
      "SPEAKER_02: 128.3s to 159.9s\n",
      "SPEAKER_01: 159.9s to 175.0s\n",
      "SPEAKER_02: 175.0s to 187.8s\n",
      "SPEAKER_02: 189.3s to 190.5s\n",
      "SPEAKER_01: 190.8s to 203.7s\n"
     ]
    }
   ],
   "source": [
    "# show each time someone spoke and output it to a csv\n",
    "\n",
    "output_file = os.path.join(output_folder, \"speaking_times.csv\")\n",
    "with open(output_file, \"w\", newline=\"\") as csvfile: \n",
    "  writer = csv.writer(csvfile)\n",
    "  #headings\n",
    "  writer.writerow(['Speaker', 'Start Time (s)', 'End Time (s)'])\n",
    "  for turn, _, speaker in diarisation.itertracks(yield_label=True):\n",
    "      writer.writerow([speaker, f\"{turn.start:.1f}\", f\"{turn.end:.1f}\"])\n",
    "      print(f\"{speaker}: {turn.start:.1f}s to {turn.end:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SPEAKER_02': [(1.70159375, 5.29596875), (20.95596875, 29.781593750000003), (52.782218750000006, 82.48221875), (100.69034375000001, 101.24721875), (103.44096875000001, 105.17909375), (128.28096875, 159.87096875), (174.95721875, 187.76534375), (189.25034375, 190.48221875000002)], 'SPEAKER_00': [(1.81971875, 2.5115937500000003), (6.64596875, 10.223468750000002), (97.02846875, 102.52971875), (106.29284375, 111.94596875), (115.62471875000001, 128.24721875)], 'SPEAKER_01': [(10.25721875, 20.61846875), (30.06846875, 52.63034375), (82.48221875, 97.02846875), (100.36971875, 100.69034375000001), (108.43596875, 115.62471875000001), (159.87096875, 174.95721875), (190.76909375000002, 203.69534375)]}\n"
     ]
    }
   ],
   "source": [
    "speaker_segments = {}\n",
    "\n",
    "# Print speaker segments\n",
    "for turn, _, speaker in diarisation.itertracks(yield_label=True):\n",
    "    if speaker not in speaker_segments:\n",
    "        speaker_segments[speaker]= []\n",
    "    speaker_segments[speaker].append((turn.start, turn.end))\n",
    "\n",
    "print(speaker_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
