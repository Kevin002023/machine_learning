{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "The first part of this project is to use pre-existing models to transcribe audio files, while identifying who was speaking and when.\n",
    "\n",
    "- Speaker Diarisation\n",
    "- Speech to Text\n",
    "- Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import os \n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pydub import AudioSegment\n",
    "import openai\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whisper requires the command line tool ffmpeg to be installed on the system. This needs to be done via a package manager like chocolatey for windows. This installs software packages at a system level. Options for other OS are listed in the README file. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kolea\\anaconda3\\Lib\\site-packages\\whisper\\__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "c:\\Users\\kolea\\anaconda3\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22381/22381 [00:21<00:00, 1033.01frames/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Thank you. Welcome to you both. It's wonderful to have you. It's an honor to have you both here tonight. We have inflation like very few people have ever seen before. Probably the worst in our nation's history. This has been a disaster for people, for the middle class, but for every class. Donald Trump left us the worst unemployment since the Great Depression. And what we have done is clean up Donald Trump's mess. She's a Marxist. Everybody knows she's a Marxist. Her father is a Marxist professor in economics. And he taught her well. But her vice presidential pick says abortion in the ninth month is absolutely fine. He also says execution after birth. It's execution no longer abortion because the baby is born is okay. And that's not okay with me. One does not have to abandon their face or deeply held back. Or deeply held beliefs to agree. The government and Donald Trump certainly should not be telling a woman what to do with her body. Pregnant women who want to carry a pregnancy to term, suffering from a miscarriage, being denied care in an emergency room because the healthcare providers are afraid they might go to jail. And she's bleeding out in a car in the parking lot. She didn't want that. Her husband didn't want that. In Springfield, they're eating the dogs. The people that came in. They're eating the cats. They're eating the pets of the people that live there. And this is what's happening in our country. Again, the Springfield City Manager says there's no evidence of that vice president here. So I'll let you respond to the rest of what you've heard. You talk about extreme. Are you now acknowledging that you've lost in 20-20? No, I don't acknowledge it at all. But, you know that. It was said, oh, we lost by our whisker. That was said sarcastically. Mr. President, thank you. Vice President Harris, you heard the president there tonight. He said he didn't say that. That he lost by whisker. So he still believes he did not lose the election. That was won by President Biden and yourself. Donald Trump was fired by 81 million people. So let's be clear about that. And clearly he is having a very difficult time processing that. World leaders are laughing at Donald Trump. I have talked with military leaders, some of whom work with you. And they say you're a disgrace. Understand what it would mean if Donald Trump were back in the White House with no guardrails. Because certainly we know now the court won't stop him. We know JD Vance is not going to stop him. It's up to the American people. We got to get you. This is the one that weaponized. Not me. She weaponized. I probably took a bullet to their head because of the things that they say about me. They talk about democracy. I'm a threat to democracy. There the threat to democracy. So I think you've heard tonight two very different visions for our country. One that is focused on the future. And the other that is focused on the past. And an attempt to take us backward. But we're not going back. They've had three and a half years to create jobs. And all the things we talked about. Why hasn't she done it? The worst president, the worst vice president in the history of our country.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#define path\n",
    "\n",
    "file_path = \"data/raw/TrumpHarrisDebate.mp3\"\n",
    "\n",
    "#select whisper model\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(file_path, verbose = False)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show contents of result\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whisper has broken down the audiofile into segments, that begin and end as each person speaks. It gives an ID number for each segment, start and end times and also the text from that segment. It also identifies what language is being used. It does not however differentiate between who is speaking when."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show language\n",
    "result['language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the data for each segment into a dataframe for visualisation\n",
    "\n",
    "#create dataframe\n",
    "all_segments_df=[]\n",
    "\n",
    "for seg in result['segments']:\n",
    "  #not interested in tokens, temperature, seek, \n",
    "  for key in ['tokens', 'seek', 'temperature']:\n",
    "        seg.pop(key, None)\n",
    "  seg_df = pd.DataFrame.from_dict({0: seg}, orient='index')\n",
    "\n",
    "  #append each segment to the dataframe\n",
    "  all_segments_df.append(seg_df)\n",
    "\n",
    "pd.concat(all_segments_df, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the transcribed text file\n",
    "\n",
    "transcription = result[\"text\"]\n",
    "\n",
    "output_folder = \"data/processed\"\n",
    "output_file = os.path.join(output_folder, \"whisper_transcription.txt\")\n",
    "with open(output_file, \"w\") as file:\n",
    "  file.write(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code has transcribed the full interview to a txt file. However it does not identify the different speakers or separate their words. To achieve this we need to use another model called pyannote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "#https://github.com/pyannote/pyannote-audio/blob/develop/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Access token to download pyannote models\n",
    "from Authtoken import token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below pyannote models take some time to download (12+minutes on my machine). To avoid this happening every time, I have directed it to be stored in the cache. Next time it runs it should pull it from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "c:\\Users\\kolea\\anaconda3\\Lib\\inspect.py:992: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\kolea\\.cache\\torch\\pyannote\\models--pyannote--segmentation\\snapshots\\c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b\\pytorch_model.bin`\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "c:\\Users\\kolea\\anaconda3\\Lib\\site-packages\\speechbrain\\utils\\fetching.py:151: UserWarning: Using SYMLINK strategy on Windows for fetching potentially requires elevated privileges and is not recommended. See `LocalStrategy` documentation.\n",
      "  warnings.warn(\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.5.1+cpu. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kolea\\anaconda3\\Lib\\site-packages\\speechbrain\\utils\\autocast.py:68: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n",
      "c:\\Users\\kolea\\anaconda3\\Lib\\site-packages\\speechbrain\\utils\\parameter_transfer.py:234: UserWarning: Requested Pretrainer collection using symlinks on Windows. This might not work; see `LocalStrategy` documentation. Consider unsetting `collect_in` in Pretrainer to avoid symlinking altogether.\n",
      "  warnings.warn(\n",
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
      "c:\\Users\\kolea\\anaconda3\\Lib\\site-packages\\speechbrain\\utils\\checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n",
      "c:\\Users\\kolea\\anaconda3\\Lib\\site-packages\\speechbrain\\processing\\features.py:1311: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stats = torch.load(path, map_location=device)\n",
      "c:\\Users\\kolea\\anaconda3\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=token, cache_dir=\"./model_cache\")\n",
    "diarisation = pipeline(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_folder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# show each time someone spoke and output it to a csv\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m output_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeaking_times.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m csvfile: \n\u001b[0;32m      5\u001b[0m   writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mwriter(csvfile)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'output_folder' is not defined"
     ]
    }
   ],
   "source": [
    "# show each time someone spoke and output it to a csv\n",
    "\n",
    "output_file = os.path.join(output_folder, \"speaking_times.csv\")\n",
    "with open(output_file, \"w\", newline=\"\") as csvfile: \n",
    "  writer = csv.writer(csvfile)\n",
    "  #headings\n",
    "  writer.writerow(['Speaker', 'Start Time (s)', 'End Time (s)'])\n",
    "  for turn, _, speaker in diarisation.itertracks(yield_label=True):\n",
    "      writer.writerow([speaker, f\"{turn.start:.1f}\", f\"{turn.end:.1f}\"])\n",
    "      print(f\"{speaker}: {turn.start:.1f}s to {turn.end:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the csv file as a Dataframe to visualise\n",
    "\n",
    "speaker_csv = \"data/processed/speaking_times.csv\"\n",
    "\n",
    "speaker_df = pd.read_csv(speaker_csv)\n",
    "speaker_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_df[\"Duration\"] = (speaker_df['End Time (s)']- speaker_df['Start Time (s)'])\n",
    "speaker_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create separate datafrmae for each speaker\n",
    "#speaker_00\n",
    "speaker_0_df = speaker_df[speaker_df[\"Speaker\"] == \"SPEAKER_00\"]\n",
    "\n",
    "#speaker_01\n",
    "speaker_1_df = speaker_df[speaker_df[\"Speaker\"] == \"SPEAKER_01\"]\n",
    "\n",
    "#speaker_02\n",
    "speaker_2_df = speaker_df[speaker_df[\"Speaker\"] == \"SPEAKER_02\"]\n",
    "\n",
    "speaker_0_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot them on bar chart to visualise the difference.\n",
    "\n",
    "total_durations = {\n",
    "  \"SPEAKER_00\": speaker_0_df[\"Duration\"].sum(),  \n",
    "  \"SPEAKER_01\": speaker_1_df[\"Duration\"].sum(),\n",
    "  \"SPEAKER_02\": speaker_2_df[\"Duration\"].sum(),\n",
    "} \n",
    "\n",
    "labels = list(total_durations.keys())\n",
    "values = list(total_durations.values())\n",
    "\n",
    "#create plot\n",
    "plt.bar(labels, values)\n",
    "\n",
    "plt.xlabel(\"Speakers\")\n",
    "plt.ylabel(\"Total Duration (seconds)\")\n",
    "plt.title(\"Total Speaking Duration per Speaker\")\n",
    "\n",
    "#Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcription\n",
    "\n",
    "As stated above Pyannote is useful for speaker diarisation but does not allow for transcribing the audio file, while whisper, transcribes but does not do speaker diarization. So we need to use a combination of the two. \n",
    "\n",
    "In order to transcribe the audio while retaining the speaker segmentation we need to split the audio file into individual segements. We can do this with the library [pydub](https://audiosegment.readthedocs.io/en/latest/audiosegment.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'diarisation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#Loop through segments\u001b[39;00m\n\u001b[0;32m      8\u001b[0m speaker_segments \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m turn, _, speaker \u001b[38;5;129;01min\u001b[39;00m diarisation\u001b[38;5;241m.\u001b[39mitertracks(yield_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m#change start and end times to milliseconds\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     segment \u001b[38;5;241m=\u001b[39m audio[turn\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1000\u001b[39m:turn\u001b[38;5;241m.\u001b[39mend \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1000\u001b[39m]\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m#pass the output folder\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'diarisation' is not defined"
     ]
    }
   ],
   "source": [
    "#load audio file\n",
    "audio = AudioSegment.from_mp3(file_path)\n",
    "\n",
    "#output folder\n",
    "segment_folder = \"data\\processed\\Segments\"\n",
    "\n",
    "#Loop through segments\n",
    "speaker_segments = []\n",
    "for turn, _, speaker in diarisation.itertracks(yield_label=True):\n",
    "    #change start and end times to milliseconds\n",
    "    segment = audio[turn.start *1000:turn.end *1000]\n",
    "\n",
    "    #pass the output folder\n",
    "    segment_file = os.path.join(segment_folder, f\"{speaker}_{int(turn.start)}_{int(turn.end)}.mp3\")    \n",
    "    segment.export(segment_file, format=\"mp3\")\n",
    "\n",
    "    speaker_segments.append({\"speaker\": speaker, \"start\": turn.start, \"end\": turn.end, \"file\": segment_file})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code has split the audio into segments based on the speaker changing. The output files are in order of speaker so when we transcribe it below, they will need to be resorted in order of start time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe Segments\n",
    "\n",
    "segment_transcriptions = []\n",
    "\n",
    "#for loop to transcribe each segment\n",
    "for segment in os.listdir(segment_folder):\n",
    "  if segment.endswith(\".mp3\"):\n",
    "        # Full path to the audio file\n",
    "        segment_path = os.path.join(segment_folder, segment)\n",
    "        \n",
    "        # Transcribe the audio segment\n",
    "        print(f\"Transcribing: {segment_path}\")\n",
    "        segment_result = model.transcribe(segment_path)\n",
    "\n",
    "        # Extract speaker, start, and end info from the filename\n",
    "        file_parts = os.path.splitext(segment)[0].split(\"_\")\n",
    "        speaker = f\"SPEAKER{file_parts[1]}\"\n",
    "        start = float(file_parts[2])\n",
    "        end = float(file_parts[3])\n",
    "\n",
    "        # Append the transcription result\n",
    "        segment_transcriptions.append({\n",
    "            \"speaker\": speaker,\n",
    "            \"start\": start,\n",
    "            \"end\": end,\n",
    "            \"text\": segment_result[\"text\"]\n",
    "        })\n",
    "\n",
    "#sort by start time to retain order of conversation\n",
    "segment_transcriptions = sorted(segment_transcriptions, key=lambda x: x['start'])\n",
    "\n",
    "# Print the results\n",
    "for transcription in segment_transcriptions:\n",
    "  print(f\"{transcription['speaker']} ({transcription['start']}-{transcription['end']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create transcribed file\n",
    "full_transcription_file = \"data/processed/full_transcription.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'segment_transcriptions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(full_transcription_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m----> 2\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m segment_transcriptions:\n\u001b[0;32m      3\u001b[0m     file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpeaker \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeaker\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated full transcription text file!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'segment_transcriptions' is not defined"
     ]
    }
   ],
   "source": [
    "#Combine transcribed files\n",
    "full_transcription_file = \"data/processed/full_transcription.txt\"\n",
    "with open(full_transcription_file, \"w\") as file:\n",
    "  for t in segment_transcriptions:\n",
    "    file.write(f\"Speaker {t['speaker']} ({t['start']:.1f}s - {t['end']:.1f}s): {t['text']}\\n\")\n",
    "\n",
    "print(\"Created full transcription text file!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cells have taken a 3 mins 43 second long audio file and broken it down to 20 individual audio segments retaining the time they start and finish speaking. They have then been transcribed individually and pasted back together to from one text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Language Models\n",
    "\n",
    "There are a number of LLMs out there (the best known being ChatGPT),that are trained on vast amounts of data to understand and generate human-like language. It can generate new text based on context allowing it to answer questions and create its own content. \n",
    "\n",
    "If we pass the transcription that we have been able to produce to an LLM we can ask it questions about it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "#hugging face have a transformer with a sentiment analysis\n",
    "\n",
    "from transformers import pipeline\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open transcript as read only form\n",
    "with open(full_transcription_file, \"r\") as file:\n",
    "  transcript= file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max tokens to be uploaded to teh sentiment analyzer is 512 so we need to split the transcript into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the transcript by lines (each line corresponds to a speaker's contribution)\n",
    "lines = transcript.split(\"\\n\")\n",
    "\n",
    "# Analyze each speaker's sentiment\n",
    "for line in lines:\n",
    "    # Skip empty lines\n",
    "    if line.strip():\n",
    "        print(f\"Line: {line}\")\n",
    "        result = sentiment_analyzer(line)\n",
    "        print(f\"Sentiment: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Hugging Face transformer](https://huggingface.co/blog/sentiment-analysis-python) library uses a pretrained model to tag data according to its sentiment, which can be either Positive, Negative or Neutral. \n",
    "\n",
    "It splits into smaller components called tokens. The tokens are then processed through a stack of transformer layers, which capture relationships between words and their contextual meaning. The output from the transformer then is fed into a classification head that maps it to a number of output classes (Positive, Negative or Neutral). The model also provides a confidence score for the predicted class. \n",
    "\n",
    "However based on quick eye ball test, we might be able to find a better model. For example it classifies the following line as Positive;\n",
    "\n",
    "\"In Springfield, they're eating the dogs, the people that came in, they're eating the cats, they're eating the pets of the people that live there. And this is what's happening in our country.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at OpenAi's ChatGPT instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87800c150b954ffea3e88aa44e74a3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0024f621fb9c48ed82c59d7214eaf909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForQuestionAnswering were not initialized from the model checkpoint at google/flan-t5-large and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01f621b03cf4983ae3472f26ee365ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5028256f63a049c18c4212b48e67a75f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a965f34d7944ad393d401797a782d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9570087a6eeb4b518c946fe29f2eb130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load a QA pipeline with a Hugging Face model\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"google/flan-t5-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello. Speaker 2:\n"
     ]
    }
   ],
   "source": [
    "question = \"can you read this?\"\n",
    "\n",
    "response = qa_pipeline(context=transcript, question=question)\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 429 {\n",
      "    \"error\": {\n",
      "        \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.\",\n",
      "        \"type\": \"insufficient_quota\",\n",
      "        \"param\": null,\n",
      "        \"code\": \"insufficient_quota\"\n",
      "    }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from Authtoken import chatgpt_token\n",
    "\n",
    "# Set your API key\n",
    "openai.api_key = chatgpt_token\n",
    "url = \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {chatgpt_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"model\": \"gpt-4o-mini\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant analyzing transcripts.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Here is the transcription:\\n\\nSpeaker 1: Hello. Speaker 2: Hi there.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can you summarize this transcript?\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Response:\", response.json()['choices'][0]['message']['content'])\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chatgpt(context, query):\n",
    "  response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages= [\n",
    "      {\"role\": \"system\", \"content\": \"You will analyze transcripts of audiofiles and answer questions about them\"},\n",
    "      {\"role\": \"user\", \"content\": f\"Here is the transcription:\\n\\n{context}\"},\n",
    "      {\"role\": \"user\", \"content\": query},\n",
    "      ]\n",
    "  )\n",
    "  return responses['choices'][0]['message']['content']\n",
    "\n",
    "user_query = \"can you read this document?\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m query_chatgpt(transcript, user_query)\n",
      "Cell \u001b[1;32mIn[30], line 2\u001b[0m, in \u001b[0;36mquery_chatgpt\u001b[1;34m(context, query)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery_chatgpt\u001b[39m(context, query):\n\u001b[1;32m----> 2\u001b[0m   response \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mChatCompletion\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     messages\u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      5\u001b[0m       {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou will analyze transcripts of audiofiles and answer questions about them\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m      6\u001b[0m       {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHere is the transcription:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m      7\u001b[0m       {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: query},\n\u001b[0;32m      8\u001b[0m       ]\n\u001b[0;32m      9\u001b[0m   )\n\u001b[0;32m     10\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m responses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\kolea\\anaconda3\\Lib\\site-packages\\openai\\lib\\_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[1;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "query_chatgpt(transcript, user_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
